{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To create a vector DB with embeddings of the codebase\n",
    "\n",
    "\n",
    "Steps:\n",
    "\n",
    "- We will upload all python project files using the langchain_community.document_loaders.TextLoader\n",
    "- iterate over the files in the LangChain repository and loads every .py file\n",
    "- splitting stragey :\n",
    "    - Keep top-level functions and classes together (into a single document)\n",
    "    - Put remaining code into a separate document\n",
    "    - Retains metadata about where each split comes from  \n",
    "\n",
    "Required libraries:\n",
    "\n",
    "langchain-openai tiktoken langchain-chroma langchain GitPython\n",
    "\n",
    "Reference: https://python.langchain.com/v0.1/docs/use_cases/code_understanding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade --quiet tiktoken langchain-chroma \n",
    "# ! pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gitchat_assistant\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv('/Users/sjr11/Work/SelfLearning/Git Chat Agent/ai_git_assistant/.env')\n",
    "print(os.getenv('LANGCHAIN_PROJECT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "from langchain_text_splitters import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository already exists at the specified path.\n"
     ]
    }
   ],
   "source": [
    "# Clone Repo locally if not already done \n",
    "repo_path = '/Users/sjr11/Work/SelfLearning/sql_python_etl/python_sql_etl'\n",
    "repo_url = \"https://github.com/shailu1309/python_sql_etl.git\"\n",
    "if not os.path.exists(repo_path):\n",
    "    # Clone the repository if it doesn't exist\n",
    "    repo = Repo.clone_from(repo_url, to_path=repo_path)\n",
    "else:\n",
    "    # Open the existing repository\n",
    "    repo = Repo(repo_path)\n",
    "    print(\"Repository already exists at the specified path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    repo_path,\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],\n",
    "    exclude=[\"**/non-utf8-encoding.py\"],\n",
    "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500),\n",
    ")\n",
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '/Users/sjr11/Work/SelfLearning/sql_python_etl/python_sql_etl/create_mock_db.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"import sqlite3\\n\\nconnection = sqlite3.connect('sqlite_mock.db')\\ncursor = connection.cursor()\\n\\ncursor.execute('''CREATE TABLE IF NOT EXISTS table1 \\n               (id INTEGER, col1 TEXT)''')\\ncursor.execute('''CREATE TABLE IF NOT EXISTS table2 \\n               (id INTEGER, col2 TEXT)''')\\n\\ncursor.executemany('INSERT INTO table1 (id, col1) VALUES (?, ?)', [\\n            (1, 'dummy'), (2, 'example'), (3, ''), (4, 'test')])\\ncursor.executemany('INSERT INTO table2 (id, col2) VALUES (?, ?)', [\\n            (1, 'value1'), (2, None), (3, 'value3'), (4, 'value4')])\\n\\n# Commit the changes to the database\\nconnection.commit()\\n\\n# Query the database to retrieve data\\ncursor.execute('SELECT * FROM table1;')\\nrows = cursor.fetchall()\\nprint(len(rows))\\nconnection\\nconnection.close()\\n\\n\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting - Split the Document into chunks for embedding and vector storage.\n",
    "# We can use RecursiveCharacterTextSplitter w/ language specified.\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=2000, chunk_overlap=200\n",
    ")\n",
    "texts = python_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating embedding with Retrieval QA\n",
    "We need to store the documents in a way we can semantically search for their content.\n",
    "\n",
    "The most common approach is to embed the contents of each document then store the embedding and document in a vector store.\n",
    "\n",
    "When setting up the vectorstore retriever:\n",
    "\n",
    "We test max marginal relevance for retrieval\n",
    "And 8 documents returned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "db = Chroma.from_documents(texts, OpenAIEmbeddings(disallowed_special=()))\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",  # Also test \"similarity\"\n",
    "    search_kwargs={\"k\": 8},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user's questions based on the below context:\\n\\n{context}\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "qa = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 5, updating n_results = 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The project appears to be about data extraction from multiple tables in a database. The script written in Python uses logging to monitor the data extraction process from three tables - table1, table2, and table3. \\n\\nThe extraction functions use SQLAlchemy to connect to a PostgreSQL database and execute queries defined in the functions `inefficient_query_1()`, `inefficient_query_2()`, and `inefficient_query_3()`. Each query returns a pandas DataFrame. \\n\\nAdditionally, an SQLite database is created and populated with data for tables 'table1' and 'table2'. \\n\\nThere's also a testing component where a pytest function is defined to compare the results of two SQL queries. \\n\\nIt appears the project might be about optimizing these 'inefficient' queries, as it includes a testing setup to compare the results of modifications to these queries.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is the project about?\"\n",
    "result = qa.invoke({\"input\": question})\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 5, updating n_results = 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: What is the project about?? \n",
      "\n",
      "**Answer**: The project is about extracting data from different tables in a database using Python. It includes creating and interacting with SQLite and PostgreSQL databases, executing SQL queries, and handling the data using pandas DataFrames. \n",
      "\n",
      "The main.py file contains the main function which logs the data extraction process and extracts data from three tables (table1, table2, table3) using the functions from the extract.py file. In case an error occurs during the extraction, it's logged and the program continues to the next table.\n",
      "\n",
      "The extract.py file contains the SQL queries for data extraction and a function to execute the SQL queries and return the data as a pandas DataFrame.\n",
      "\n",
      "There's also a test module that uses pytest to compare the results of two SQL queries to ensure they return the same results. This is done by parameterizing the test function with different pairs of SQL queries. \n",
      "\n",
      "The project also includes creating tables and inserting data into them in the SQLite database. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 5, updating n_results = 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: Are there sql queries in the project? \n",
      "\n",
      "**Answer**: Yes, there are SQL queries in the project. These queries are used to interact with the databases 'sqlite_mock.db' and 'dummy_postgres'. For example, queries are used to create tables, insert data into tables, and retrieve data from tables. There are also inefficient queries defined in the 'extract.py' file which are used to extract data from the tables in 'dummy_postgres'. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 5, updating n_results = 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: Can you identify the file names where sql queries are written in the project? \n",
      "\n",
      "**Answer**: The SQL queries are written in two files: \n",
      "\n",
      "1. In the initial part of the context, there are SQL queries in an unnamed file, which could be a test file based on the usage of pytest.\n",
      "  \n",
      "2. The second file is \"extract.py\" where inefficient queries for table1, table2, and table3 are defined. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is the project about??\",\n",
    "    \"Are there sql queries in the project?\",\n",
    "    \"Can you identify the file names where sql queries are written in the project?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = qa.invoke({\"input\": question})\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_gchat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
